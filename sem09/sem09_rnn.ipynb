{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1W8R8WgZceEk"
   },
   "source": [
    "# Character-Level LSTM in PyTorch\n",
    "\n",
    "На этом семинаре поговорим про RNN. Здесь мы обучим модель на тексте Анны Карениной, а затем будем генерировать новый текст.**Эта модель сможет генерировать новый текст на основе нашего текста!**\n",
    "\n",
    "Можно посмотреть [очень крутую статью про RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/), а также [не менее крутуя статью про LSTM](http://colah.github.io/posts/2015-08-Understanding-LSTMs/). Ниже представлена ​​общая архитектура RNN.\n",
    "\n",
    "<img src=\"https://github.com/udacity/deep-learning-v2-pytorch/blob/master/recurrent-neural-networks/char-rnn/assets/charseq.jpeg?raw=1\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RMb9A1BIceEl"
   },
   "source": [
    "Сначала загрузим ресурсы, необходимые для загрузки данных и создания модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "sqUOE2flceEl"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_wHfCDyzceEl"
   },
   "source": [
    "## Load in Data\n",
    "\n",
    "Затем мы загрузим текстовый файл Анны Карениной."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "b34kfqIOceEl"
   },
   "outputs": [],
   "source": [
    "# open text file and read in data as `text`\n",
    "with open('anna.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jp1Ljc4mceEl"
   },
   "source": [
    "Давайте проверим первые 100 символов, убедимся, что все красиво. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "7VctmLQfceEl",
    "outputId": "a8c89cbe-6561-45e6-cca5-103c57a965bf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1985223"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4iC21bopceEl"
   },
   "source": [
    "### Tokenization\n",
    "\n",
    "В ячейках ниже постараемся создать пару **словарей** для преобразования символов в целые числа и обратно. Кодирование символов как целых чисел упрощает их использование в качестве входных данных в сети."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "tYVlmnxLceEl"
   },
   "outputs": [],
   "source": [
    "# encode the text and map each character to an integer and vice versa\n",
    "\n",
    "# we create two dictionaries:\n",
    "# 1. int2char, which maps integers to characters\n",
    "# 2. char2int, which maps characters to unique integers\n",
    "chars = tuple(set(text))\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "\n",
    "# encode the text\n",
    "encoded = np.array([char2int[ch] for ch in text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oJIzwzSwceEl"
   },
   "source": [
    "И мы можем видеть те же самые символы сверху, закодированные как целые числа."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WK1MYr_9ceEl",
    "outputId": "7ca35004-794b-4849-e526-980c7fd915be"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([23, 17, 14, 79, 58, 19, 10,  0, 48, 78, 78, 78, 82, 14, 79, 79, 25,\n",
       "        0, 66, 14, 32, 63, 62, 63, 19, 29,  0, 14, 10, 19,  0, 14, 62, 62,\n",
       "        0, 14, 62, 63,  5, 19,  3,  0, 19, 71, 19, 10, 25,  0, 77, 35, 17,\n",
       "       14, 79, 79, 25,  0, 66, 14, 32, 63, 62, 25,  0, 63, 29,  0, 77, 35,\n",
       "       17, 14, 79, 79, 25,  0, 63, 35,  0, 63, 58, 29,  0, 60, 73, 35, 78,\n",
       "       73, 14, 25, 37, 78, 78, 27, 71, 19, 10, 25, 58, 17, 63, 35])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "azltQy-gceEl"
   },
   "source": [
    "## Pre-processing the data\n",
    "\n",
    "Как вы можете видеть на изображении char-RNN выше, наш LSTM ожидает ввода, который  **one-hot encoded** , что означает, что каждый символ преобразуется в целое число (через наш созданный словарь), а затем преобразуется в столбец вектор, где только соответствующий ему целочисленный индекс будет иметь значение 1, а остальная часть вектора будет заполнена нулями. Давайте создадим для этого функцию!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "OnahALhiceEl"
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels):\n",
    "    \n",
    "    # Initialize the the encoded array\n",
    "    one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n",
    "    \n",
    "    # Fill the appropriate elements with ones\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    \n",
    "    # Finally reshape it to get back to the original array\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L3lTdLKfceEl",
    "outputId": "563d6309-b450-4f8d-f6c7-eee96be05d61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 1. 0. 0. 0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# check that the function works as expected\n",
    "test_seq = np.array([[3, 5, 1]])\n",
    "one_hot = one_hot_encode(test_seq, 8)\n",
    "\n",
    "print(one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9YyL91CuceEl"
   },
   "source": [
    "## Making training mini-batches\n",
    "\n",
    "\n",
    "Для обучения на этих данных нужно создать мини-батчи для обучения. На простом примере наши батчи будут выглядеть так:\n",
    "\n",
    "<img src=\"https://github.com/udacity/deep-learning-v2-pytorch/blob/master/recurrent-neural-networks/char-rnn/assets/sequence_batching@1x.png?raw=1\" width=500px>\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "В этом примере возьмем закодированные символы (переданные как параметр arr) и разделим их на несколько последовательностей, заданных параметром batch_size. Каждая из наших последовательностей будет иметь длину seq_length.\n",
    "\n",
    "### Creating Batches\n",
    "\n",
    "**1. Первое, что нам нужно сделать, это отбросить часть текста, чтобы у нас были только полные мини-батчи.**\n",
    "\n",
    "Каждый батч содержит $ N\\times M $ символов, где $ N $ - это размер батча (количество последовательностей в батче), а $ M $ - длина seq_length или количество шагов в последовательности. Затем, чтобы получить общее количество батчей $ K $, которое мы можем сделать из массива arr, нужно разделить длину arr на количество символов в батче. Как только вы знаете количество пакетов, вы можете получить общее количество символов, которые нужно сохранить, из `arr`, $ N * M * K $.\n",
    "\n",
    "**2. После этого нам нужно разделить arr на $N$ пакетов.** \n",
    "\n",
    "Вы можете сделать это с помощью `arr.reshape(size)`, где `size` - это кортеж, содержащий размеры измененного массива. Мы знаем, что нам нужно $ N $ последовательностей в батче, поэтому сделаем его размером первого измерения. Для второго измерения можем использовать «-1» в качестве заполнителя, он заполнит массив соответствующими данными. После этого должен остаться массив $N\\times(M * K)$.\n",
    "\n",
    "**3. Теперь, когда у нас есть этот массив, мы можем перебирать его, чтобы получить наши мини-батчи.**\n",
    "\n",
    "Идея состоит в том, что каждая партия представляет собой окно $ N\\times M $ в массиве $ N\\times (M * K) $. Для каждого последующего батча окно перемещается на seq_length. Мы также хотим создать как входной, так и выходной массивы. Это окно можно сделать с помощью `range`, чтобы делать шаги размером `n_steps` от $ 0 $ до `arr.shape [1]`, общее количество токенов в каждой последовательности. Таким образом, целые числа, которые получены из диапазона, всегда указывают на начало батча, и каждое окно имеет ширину seq_length.\n",
    "\n",
    "> **TODO:** Напишите код для создания батчей в функции ниже. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 130
    },
    "id": "8vmDKLiOceEl",
    "outputId": "a58e9fbd-1548-4a7d-8217-a03f6be262ec"
   },
   "outputs": [],
   "source": [
    "def get_batches(arr, batch_size, seq_length):\n",
    "    '''Create a generator that returns batches of size\n",
    "       batch_size x seq_length from arr.\n",
    "       \n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array you want to make batches from\n",
    "       batch_size: Batch size, the number of sequences per batch\n",
    "       seq_length: Number of encoded chars in a sequence\n",
    "    '''\n",
    "    \n",
    "    ## TODO: Get the number of batches we can make\n",
    "    batch_size_total = ...\n",
    "    n_batches = ...\n",
    "    \n",
    "    ## TODO: Keep only enough characters to make full batches\n",
    "    arr = ...\n",
    "    \n",
    "    ## TODO: Reshape into batch_size rows\n",
    "    arr = ...\n",
    "\n",
    "    for n in range(0, arr.shape[1], seq_length):\n",
    "        # The features\n",
    "        x = arr[:, n:n+seq_length]\n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9uKOvbqceEl"
   },
   "source": [
    "### Test Your Implementation\n",
    "\n",
    "Теперь создадим несколько наборов данных, и проверим, что происходит, когда мы пакетируем данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[40, 79, 77, 79,  5, 17, 23,  4, 44,  5],\n",
       "       [73, 23, 43, 33,  5, 23, 66, 14, 77, 29],\n",
       "       [11, 44,  5, 73, 47, 33, 54,  7, 79, 44],\n",
       "       [23, 47, 29, 13, 77, 68, 23, 73, 29, 43],\n",
       "       [78, 17, 45, 23,  4, 73, 68, 23, 73, 29],\n",
       "       [23,  2,  5, 44,  5, 23, 39,  4, 79, 73],\n",
       "       [79, 73,  7, 23, 43, 29,  7,  5, 43, 33],\n",
       "       [33, 23, 29, 73,  5, 23,  4, 73, 29, 43]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[:8*100].reshape((8, -1))[:, 20:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "qtKlLXi1ceEl"
   },
   "outputs": [],
   "source": [
    "batches = get_batches(encoded, 8, 50)\n",
    "x, y = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rg5MUTqqceEl",
    "outputId": "13922dfb-8ff3-436f-a914-90cc4114be91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[22 33  4 39 43  5 44 23 32 54]\n",
      " [17 29 73 23 43 33  4 43 23  4]\n",
      " [ 5 73 68 23 29 44 23  4 23  3]\n",
      " [17 23 43 33  5 23 47 33 79  5]\n",
      " [23 17  4  2 23 33  5 44 23 43]\n",
      " [47 13 17 17 79 29 73 23  4 73]\n",
      " [23 16 73 73  4 23 33  4 68 23]\n",
      " [66 14 77 29 73 17 51 78 24 23]]\n",
      "\n",
      "y\n",
      " [[33  4 39 43  5 44 23 32 54 54]\n",
      " [29 73 23 43 33  4 43 23  4 43]\n",
      " [73 68 23 29 44 23  4 23  3 29]\n",
      " [23 43 33  5 23 47 33 79  5  3]\n",
      " [17  4  2 23 33  5 44 23 43  5]\n",
      " [13 17 17 79 29 73 23  4 73 68]\n",
      " [16 73 73  4 23 33  4 68 23 17]\n",
      " [14 77 29 73 17 51 78 24 23 52]]\n"
     ]
    }
   ],
   "source": [
    "# printing out the first 10 items in a sequence\n",
    "print('x\\n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R_qHIAEIceEl"
   },
   "source": [
    "Если вы правильно реализовали get_batches, результат должен выглядеть примерно так:\n",
    "```\n",
    "x\n",
    " [[25  8 60 11 45 27 28 73  1  2]\n",
    " [17  7 20 73 45  8 60 45 73 60]\n",
    " [27 20 80 73  7 28 73 60 73 65]\n",
    " [17 73 45  8 27 73 66  8 46 27]\n",
    " [73 17 60 12 73  8 27 28 73 45]\n",
    " [66 64 17 17 46  7 20 73 60 20]\n",
    " [73 76 20 20 60 73  8 60 80 73]\n",
    " [47 35 43  7 20 17 24 50 37 73]]\n",
    "\n",
    "y\n",
    " [[ 8 60 11 45 27 28 73  1  2  2]\n",
    " [ 7 20 73 45  8 60 45 73 60 45]\n",
    " [20 80 73  7 28 73 60 73 65  7]\n",
    " [73 45  8 27 73 66  8 46 27 65]\n",
    " [17 60 12 73  8 27 28 73 45 27]\n",
    " [64 17 17 46  7 20 73 60 20 80]\n",
    " [76 20 20 60 73  8 60 80 73 17]\n",
    " [35 43  7 20 17 24 50 37 73 36]]\n",
    " ```\n",
    " хотя точные цифры могут отличаться. Убедитесь, что данные сдвинуты на один шаг для `y`!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jouxv0L2ceEl"
   },
   "source": [
    "---\n",
    "## Defining the network with PyTorch\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/udacity/deep-learning-v2-pytorch/blob/master/recurrent-neural-networks/char-rnn/assets/charRNN.png?raw=1\" width=500px>\n",
    "\n",
    "Затем используем PyTorch для определения архитектуры сети. Начнем с определения слоев и операций, методов прямого прохода. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E7s5eRaoceEl"
   },
   "source": [
    "### Model Structure\n",
    "\n",
    "В `__init__` предлагаемая структура выглядит следующим образом:\n",
    "* Создавать и хранить необходимые словари (это было сделано за вас)\n",
    "* Определите слой LSTM, который принимает в качестве параметров: размер ввода (количество символов), размер скрытого слоя `n_hidden`, количество слоев` n_layers`, вероятность выпадения `drop_prob` и логическое значение batch_first (True)\n",
    "* Определите слой отброса данных с помощью drop_prob\n",
    "* Определите полносвязанный слой с параметрами: размер ввода `n_hidden` и размер выхода - количество символов\n",
    "* Наконец, инициализируйте веса\n",
    "\n",
    "Обратите внимание, что некоторые параметры были названы и указаны в функции `__init__`, их нужно сохранить и использовать, выполняя что-то вроде` self.drop_prob = drop_prob`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Plm1atCuceEl"
   },
   "source": [
    "---\n",
    "### LSTM Inputs/Outputs\n",
    "\n",
    "Вы можете создать [LSTM layer](https://pytorch.org/docs/stable/nn.html#lstm) следующим образом\n",
    "\n",
    "```python\n",
    "self.lstm = nn.LSTM(input_size, n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "```\n",
    "\n",
    "где input_size - это количество символов, которые эта ячейка ожидает видеть в качестве последовательного ввода, а n_hidden - это количество единиц в скрытых слоях ячейки. Можно добавить выпадение, добавив параметр выпадения с заданной вероятностью; это автоматически добавит отсев на входах или выходах. Наконец, в функции `forward` мы можем складывать ячейки LSTM в слои, используя `.view`. При этом он отправляет вывод одной ячейки в следующую ячейку.\n",
    "\n",
    "Здесь же требуется создать начальное скрытое состояние всех нулей:\n",
    "\n",
    "```python\n",
    "self.init_hidden()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HlTnDntHceEl",
    "outputId": "4c62b499-ea02-446b-db33-f1833de9d6bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU!\n"
     ]
    }
   ],
   "source": [
    "# check if GPU is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU!')\n",
    "else: \n",
    "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "VPq1EA38rBqn"
   },
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, tokens, n_hidden=256, n_layers=2,\n",
    "                               drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        # creating character dictionaries\n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        ## TODO: define the LSTM\n",
    "        self.lstm = nn.LSTM(#YOUR CODDE HERE,\n",
    "                            n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        ## TODO: define a dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        ## TODO: define the final, fully-connected output layer\n",
    "        self.fc = nn.Linear(\n",
    "            # YOUR CODE HERE\n",
    "        )\n",
    "      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "                \n",
    "        ## TODO: Get the outputs and the new hidden state from the lstm\n",
    "        r_output, hidden = ...\n",
    "        \n",
    "        ## TODO: pass through a dropout layer\n",
    "        out = ...\n",
    "        \n",
    "        # Stack up LSTM outputs using view\n",
    "        # you may need to use contiguous to reshape the output\n",
    "        out = out.contiguous().view(-1, self.n_hidden)\n",
    "        \n",
    "        ## TODO: put x through the fully-connected layer\n",
    "        out = ...\n",
    "        \n",
    "        # return the final output and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5IrBRlEPceEl"
   },
   "source": [
    "## Time to train\n",
    "\n",
    "Во время обучения нужно установить количество эпох, скорость обучения и другие параметры.\n",
    "\n",
    "Используем оптимизатор Адама и кросс-энтропию, считаем loss и, как обычно, выполняем back propagation!\n",
    "\n",
    "Пара подробностей об обучении:\n",
    "> * В рамках цикла мы отделяем скрытое состояние от его истории; на этот раз установив его равным новой переменной * tuple *, потому что LSTM имеет скрытое состояние, которое является кортежем скрытых состояний.\n",
    "* Мы используем [`clip_grad_norm_`](https://pytorch.org/docs/stable/_modules/torch/nn/utils/clip_grad.html) чтобы избавиться от взрывающегося градиента."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "lv8VkRI0ceEl"
   },
   "outputs": [],
   "source": [
    "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
    "    ''' Training a network \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        net: CharRNN network\n",
    "        data: text data to train the network\n",
    "        epochs: Number of epochs to train\n",
    "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
    "        seq_length: Number of character steps per mini-batch\n",
    "        lr: learning rate\n",
    "        clip: gradient clipping\n",
    "        val_frac: Fraction of data to hold out for validation\n",
    "        print_every: Number of steps for printing training and validation loss\n",
    "    \n",
    "    '''\n",
    "    net.train()\n",
    "    \n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # create training and validation data\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    \n",
    "    counter = 0\n",
    "    n_chars = len(net.chars)\n",
    "    for e in range(epochs):\n",
    "        # initialize hidden state\n",
    "        h = net.init_hidden(batch_size)\n",
    "        \n",
    "        for x, y in get_batches(data, batch_size, seq_length):\n",
    "            counter += 1\n",
    "            \n",
    "            # One-hot encode our data and make them Torch tensors\n",
    "            x = one_hot_encode(x, n_chars)\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            if counter == 2:\n",
    "                print(inputs.shape, targets.shape)\n",
    "                print(inputs, targets)\n",
    "            \n",
    "            if(train_on_gpu):\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "            \n",
    "            # get the output from the model\n",
    "            output, h = net(inputs, h)\n",
    "            \n",
    "            # calculate the loss and perform backprop\n",
    "            loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "            loss.backward()\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            opt.step()\n",
    "            \n",
    "            # loss stats\n",
    "            if counter % print_every == 0:\n",
    "                # Get validation loss\n",
    "                val_h = net.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "                net.eval()\n",
    "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
    "                    # One-hot encode our data and make them Torch tensors\n",
    "                    x = one_hot_encode(x, n_chars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    \n",
    "                    inputs, targets = x, y\n",
    "                    if(train_on_gpu):\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "                    output, val_h = net(inputs, val_h)\n",
    "                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "                \n",
    "                    val_losses.append(val_loss.item())\n",
    "                \n",
    "                net.train() # reset to train mode after iterationg through validation data\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gt0q4KGEceEm"
   },
   "source": [
    "## Instantiating the model\n",
    "\n",
    "Теперь мы можем создать модель с заданными гиперпараметрами. Определим размеры мини-батчей!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ykMcIloEr3G7",
    "outputId": "0bc5a222-f3ae-4632-b190-c107f6bfba38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=512, out_features=83, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# define and print the net\n",
    "n_hidden=512\n",
    "n_layers=2\n",
    "\n",
    "net = CharRNN(chars, n_hidden, n_layers)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XHy6mECuceEm"
   },
   "source": [
    "### Set your training hyperparameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 392
    },
    "id": "8hTkNrWEsjgI",
    "outputId": "197e9c85-4733-49fd-afc9-8c195d03fcde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 100, 83]) torch.Size([128, 100])\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]]) tensor([[ 0, 73, 14,  ..., 60, 35,  0],\n",
      "        [ 0, 29, 79,  ..., 19,  0, 32],\n",
      "        [35, 19, 63,  ..., 58, 19,  0],\n",
      "        ...,\n",
      "        [17,  7,  0,  ..., 25, 78, 11],\n",
      "        [62, 60, 73,  ..., 32, 19,  0],\n",
      "        [25, 19, 71,  ...,  0, 11, 62]])\n",
      "Epoch: 1/20... Step: 10... Loss: 3.2621... Val Loss: 3.2030\n",
      "Epoch: 1/20... Step: 20... Loss: 3.1576... Val Loss: 3.1371\n",
      "Epoch: 1/20... Step: 30... Loss: 3.1422... Val Loss: 3.1239\n",
      "Epoch: 1/20... Step: 40... Loss: 3.1157... Val Loss: 3.1204\n",
      "Epoch: 1/20... Step: 50... Loss: 3.1435... Val Loss: 3.1183\n",
      "Epoch: 1/20... Step: 60... Loss: 3.1195... Val Loss: 3.1167\n",
      "Epoch: 1/20... Step: 70... Loss: 3.1109... Val Loss: 3.1165\n",
      "Epoch: 1/20... Step: 80... Loss: 3.1271... Val Loss: 3.1150\n",
      "Epoch: 1/20... Step: 90... Loss: 3.1273... Val Loss: 3.1115\n",
      "Epoch: 1/20... Step: 100... Loss: 3.1138... Val Loss: 3.1060\n",
      "Epoch: 1/20... Step: 110... Loss: 3.1061... Val Loss: 3.0924\n",
      "Epoch: 1/20... Step: 120... Loss: 3.0586... Val Loss: 3.0587\n",
      "Epoch: 1/20... Step: 130... Loss: 3.0142... Val Loss: 2.9863\n",
      "Epoch: 2/20... Step: 140... Loss: 2.9134... Val Loss: 2.8607\n",
      "Epoch: 2/20... Step: 150... Loss: 2.8452... Val Loss: 2.7971\n",
      "Epoch: 2/20... Step: 160... Loss: 2.7235... Val Loss: 2.6833\n",
      "Epoch: 2/20... Step: 170... Loss: 2.6098... Val Loss: 2.5982\n",
      "Epoch: 2/20... Step: 180... Loss: 2.5613... Val Loss: 2.5345\n",
      "Epoch: 2/20... Step: 190... Loss: 2.4927... Val Loss: 2.4768\n",
      "Epoch: 2/20... Step: 200... Loss: 2.4845... Val Loss: 2.4524\n",
      "Epoch: 2/20... Step: 210... Loss: 2.4397... Val Loss: 2.4155\n",
      "Epoch: 2/20... Step: 220... Loss: 2.4058... Val Loss: 2.3820\n",
      "Epoch: 2/20... Step: 230... Loss: 2.3841... Val Loss: 2.3529\n",
      "Epoch: 2/20... Step: 240... Loss: 2.3729... Val Loss: 2.3285\n",
      "Epoch: 2/20... Step: 250... Loss: 2.3073... Val Loss: 2.3074\n",
      "Epoch: 2/20... Step: 260... Loss: 2.2920... Val Loss: 2.2793\n",
      "Epoch: 2/20... Step: 270... Loss: 2.2844... Val Loss: 2.2542\n",
      "Epoch: 3/20... Step: 280... Loss: 2.2850... Val Loss: 2.2308\n",
      "Epoch: 3/20... Step: 290... Loss: 2.2433... Val Loss: 2.2065\n",
      "Epoch: 3/20... Step: 300... Loss: 2.2142... Val Loss: 2.1846\n",
      "Epoch: 3/20... Step: 310... Loss: 2.1980... Val Loss: 2.1656\n",
      "Epoch: 3/20... Step: 320... Loss: 2.1637... Val Loss: 2.1419\n",
      "Epoch: 3/20... Step: 330... Loss: 2.1319... Val Loss: 2.1305\n",
      "Epoch: 3/20... Step: 340... Loss: 2.1459... Val Loss: 2.1101\n",
      "Epoch: 3/20... Step: 350... Loss: 2.1390... Val Loss: 2.0892\n",
      "Epoch: 3/20... Step: 360... Loss: 2.0647... Val Loss: 2.0711\n",
      "Epoch: 3/20... Step: 370... Loss: 2.0881... Val Loss: 2.0523\n",
      "Epoch: 3/20... Step: 380... Loss: 2.0551... Val Loss: 2.0339\n",
      "Epoch: 3/20... Step: 390... Loss: 2.0271... Val Loss: 2.0170\n",
      "Epoch: 3/20... Step: 400... Loss: 2.0058... Val Loss: 1.9991\n",
      "Epoch: 3/20... Step: 410... Loss: 2.0142... Val Loss: 1.9881\n",
      "Epoch: 4/20... Step: 420... Loss: 2.0086... Val Loss: 1.9675\n",
      "Epoch: 4/20... Step: 430... Loss: 1.9915... Val Loss: 1.9534\n",
      "Epoch: 4/20... Step: 440... Loss: 1.9836... Val Loss: 1.9452\n",
      "Epoch: 4/20... Step: 450... Loss: 1.9133... Val Loss: 1.9272\n",
      "Epoch: 4/20... Step: 460... Loss: 1.8984... Val Loss: 1.9120\n",
      "Epoch: 4/20... Step: 470... Loss: 1.9317... Val Loss: 1.9013\n",
      "Epoch: 4/20... Step: 480... Loss: 1.9066... Val Loss: 1.8892\n",
      "Epoch: 4/20... Step: 490... Loss: 1.9113... Val Loss: 1.8759\n",
      "Epoch: 4/20... Step: 500... Loss: 1.9094... Val Loss: 1.8648\n",
      "Epoch: 4/20... Step: 510... Loss: 1.8758... Val Loss: 1.8513\n",
      "Epoch: 4/20... Step: 520... Loss: 1.9024... Val Loss: 1.8401\n",
      "Epoch: 4/20... Step: 530... Loss: 1.8545... Val Loss: 1.8321\n",
      "Epoch: 4/20... Step: 540... Loss: 1.8070... Val Loss: 1.8154\n",
      "Epoch: 4/20... Step: 550... Loss: 1.8543... Val Loss: 1.8055\n",
      "Epoch: 5/20... Step: 560... Loss: 1.8238... Val Loss: 1.7920\n",
      "Epoch: 5/20... Step: 570... Loss: 1.8005... Val Loss: 1.7823\n",
      "Epoch: 5/20... Step: 580... Loss: 1.7790... Val Loss: 1.7763\n",
      "Epoch: 5/20... Step: 590... Loss: 1.7903... Val Loss: 1.7658\n",
      "Epoch: 5/20... Step: 600... Loss: 1.7704... Val Loss: 1.7552\n",
      "Epoch: 5/20... Step: 610... Loss: 1.7677... Val Loss: 1.7479\n",
      "Epoch: 5/20... Step: 620... Loss: 1.7563... Val Loss: 1.7436\n",
      "Epoch: 5/20... Step: 630... Loss: 1.7725... Val Loss: 1.7307\n",
      "Epoch: 5/20... Step: 640... Loss: 1.7434... Val Loss: 1.7224\n",
      "Epoch: 5/20... Step: 650... Loss: 1.7333... Val Loss: 1.7176\n",
      "Epoch: 5/20... Step: 660... Loss: 1.6950... Val Loss: 1.7077\n",
      "Epoch: 5/20... Step: 670... Loss: 1.7339... Val Loss: 1.7014\n",
      "Epoch: 5/20... Step: 680... Loss: 1.7266... Val Loss: 1.6917\n",
      "Epoch: 5/20... Step: 690... Loss: 1.7009... Val Loss: 1.6825\n",
      "Epoch: 6/20... Step: 700... Loss: 1.6918... Val Loss: 1.6802\n",
      "Epoch: 6/20... Step: 710... Loss: 1.6860... Val Loss: 1.6720\n",
      "Epoch: 6/20... Step: 720... Loss: 1.6786... Val Loss: 1.6611\n",
      "Epoch: 6/20... Step: 730... Loss: 1.6841... Val Loss: 1.6609\n",
      "Epoch: 6/20... Step: 740... Loss: 1.6445... Val Loss: 1.6526\n",
      "Epoch: 6/20... Step: 750... Loss: 1.6377... Val Loss: 1.6467\n",
      "Epoch: 6/20... Step: 760... Loss: 1.6704... Val Loss: 1.6407\n",
      "Epoch: 6/20... Step: 770... Loss: 1.6585... Val Loss: 1.6357\n",
      "Epoch: 6/20... Step: 780... Loss: 1.6357... Val Loss: 1.6278\n",
      "Epoch: 6/20... Step: 790... Loss: 1.6249... Val Loss: 1.6244\n",
      "Epoch: 6/20... Step: 800... Loss: 1.6384... Val Loss: 1.6149\n",
      "Epoch: 6/20... Step: 810... Loss: 1.6189... Val Loss: 1.6123\n",
      "Epoch: 6/20... Step: 820... Loss: 1.5821... Val Loss: 1.6067\n",
      "Epoch: 6/20... Step: 830... Loss: 1.6387... Val Loss: 1.6003\n",
      "Epoch: 7/20... Step: 840... Loss: 1.5897... Val Loss: 1.5970\n",
      "Epoch: 7/20... Step: 850... Loss: 1.6035... Val Loss: 1.5907\n",
      "Epoch: 7/20... Step: 860... Loss: 1.5958... Val Loss: 1.5846\n",
      "Epoch: 7/20... Step: 870... Loss: 1.6020... Val Loss: 1.5815\n",
      "Epoch: 7/20... Step: 880... Loss: 1.5929... Val Loss: 1.5775\n",
      "Epoch: 7/20... Step: 890... Loss: 1.5960... Val Loss: 1.5709\n",
      "Epoch: 7/20... Step: 900... Loss: 1.5797... Val Loss: 1.5684\n",
      "Epoch: 7/20... Step: 910... Loss: 1.5408... Val Loss: 1.5665\n",
      "Epoch: 7/20... Step: 920... Loss: 1.5716... Val Loss: 1.5633\n",
      "Epoch: 7/20... Step: 930... Loss: 1.5586... Val Loss: 1.5564\n",
      "Epoch: 7/20... Step: 940... Loss: 1.5546... Val Loss: 1.5519\n",
      "Epoch: 7/20... Step: 950... Loss: 1.5613... Val Loss: 1.5462\n",
      "Epoch: 7/20... Step: 960... Loss: 1.5571... Val Loss: 1.5423\n",
      "Epoch: 7/20... Step: 970... Loss: 1.5703... Val Loss: 1.5385\n",
      "Epoch: 8/20... Step: 980... Loss: 1.5393... Val Loss: 1.5361\n",
      "Epoch: 8/20... Step: 990... Loss: 1.5419... Val Loss: 1.5311\n",
      "Epoch: 8/20... Step: 1000... Loss: 1.5346... Val Loss: 1.5277\n",
      "Epoch: 8/20... Step: 1010... Loss: 1.5669... Val Loss: 1.5280\n",
      "Epoch: 8/20... Step: 1020... Loss: 1.5474... Val Loss: 1.5238\n",
      "Epoch: 8/20... Step: 1030... Loss: 1.5229... Val Loss: 1.5168\n",
      "Epoch: 8/20... Step: 1040... Loss: 1.5434... Val Loss: 1.5152\n",
      "Epoch: 8/20... Step: 1050... Loss: 1.4954... Val Loss: 1.5125\n",
      "Epoch: 8/20... Step: 1060... Loss: 1.5069... Val Loss: 1.5113\n",
      "Epoch: 8/20... Step: 1070... Loss: 1.5200... Val Loss: 1.5067\n",
      "Epoch: 8/20... Step: 1080... Loss: 1.5197... Val Loss: 1.5006\n",
      "Epoch: 8/20... Step: 1090... Loss: 1.5003... Val Loss: 1.4986\n",
      "Epoch: 8/20... Step: 1100... Loss: 1.4892... Val Loss: 1.4940\n",
      "Epoch: 8/20... Step: 1110... Loss: 1.4940... Val Loss: 1.4957\n",
      "Epoch: 9/20... Step: 1120... Loss: 1.5040... Val Loss: 1.4954\n",
      "Epoch: 9/20... Step: 1130... Loss: 1.5001... Val Loss: 1.4861\n",
      "Epoch: 9/20... Step: 1140... Loss: 1.4979... Val Loss: 1.4839\n",
      "Epoch: 9/20... Step: 1150... Loss: 1.5159... Val Loss: 1.4804\n",
      "Epoch: 9/20... Step: 1160... Loss: 1.4609... Val Loss: 1.4818\n",
      "Epoch: 9/20... Step: 1170... Loss: 1.4750... Val Loss: 1.4774\n",
      "Epoch: 9/20... Step: 1180... Loss: 1.4757... Val Loss: 1.4768\n",
      "Epoch: 9/20... Step: 1190... Loss: 1.5065... Val Loss: 1.4695\n",
      "Epoch: 9/20... Step: 1200... Loss: 1.4511... Val Loss: 1.4684\n",
      "Epoch: 9/20... Step: 1210... Loss: 1.4622... Val Loss: 1.4682\n",
      "Epoch: 9/20... Step: 1220... Loss: 1.4638... Val Loss: 1.4630\n",
      "Epoch: 9/20... Step: 1230... Loss: 1.4461... Val Loss: 1.4632\n",
      "Epoch: 9/20... Step: 1240... Loss: 1.4511... Val Loss: 1.4584\n",
      "Epoch: 9/20... Step: 1250... Loss: 1.4649... Val Loss: 1.4591\n",
      "Epoch: 10/20... Step: 1260... Loss: 1.4613... Val Loss: 1.4566\n",
      "Epoch: 10/20... Step: 1270... Loss: 1.4617... Val Loss: 1.4542\n",
      "Epoch: 10/20... Step: 1280... Loss: 1.4767... Val Loss: 1.4491\n",
      "Epoch: 10/20... Step: 1290... Loss: 1.4528... Val Loss: 1.4443\n",
      "Epoch: 10/20... Step: 1300... Loss: 1.4499... Val Loss: 1.4455\n",
      "Epoch: 10/20... Step: 1310... Loss: 1.4615... Val Loss: 1.4428\n",
      "Epoch: 10/20... Step: 1320... Loss: 1.4204... Val Loss: 1.4438\n",
      "Epoch: 10/20... Step: 1330... Loss: 1.4293... Val Loss: 1.4388\n",
      "Epoch: 10/20... Step: 1340... Loss: 1.4170... Val Loss: 1.4414\n",
      "Epoch: 10/20... Step: 1350... Loss: 1.4092... Val Loss: 1.4353\n",
      "Epoch: 10/20... Step: 1360... Loss: 1.4184... Val Loss: 1.4319\n",
      "Epoch: 10/20... Step: 1370... Loss: 1.4103... Val Loss: 1.4347\n",
      "Epoch: 10/20... Step: 1380... Loss: 1.4431... Val Loss: 1.4287\n",
      "Epoch: 10/20... Step: 1390... Loss: 1.4466... Val Loss: 1.4292\n",
      "Epoch: 11/20... Step: 1400... Loss: 1.4429... Val Loss: 1.4292\n",
      "Epoch: 11/20... Step: 1410... Loss: 1.4621... Val Loss: 1.4231\n",
      "Epoch: 11/20... Step: 1420... Loss: 1.4413... Val Loss: 1.4190\n",
      "Epoch: 11/20... Step: 1430... Loss: 1.4126... Val Loss: 1.4231\n",
      "Epoch: 11/20... Step: 1440... Loss: 1.4447... Val Loss: 1.4165\n",
      "Epoch: 11/20... Step: 1450... Loss: 1.3672... Val Loss: 1.4154\n",
      "Epoch: 11/20... Step: 1460... Loss: 1.4027... Val Loss: 1.4183\n",
      "Epoch: 11/20... Step: 1470... Loss: 1.3946... Val Loss: 1.4146\n",
      "Epoch: 11/20... Step: 1480... Loss: 1.4061... Val Loss: 1.4102\n",
      "Epoch: 11/20... Step: 1490... Loss: 1.4051... Val Loss: 1.4091\n",
      "Epoch: 11/20... Step: 1500... Loss: 1.3897... Val Loss: 1.4112\n",
      "Epoch: 11/20... Step: 1510... Loss: 1.3677... Val Loss: 1.4089\n",
      "Epoch: 11/20... Step: 1520... Loss: 1.4023... Val Loss: 1.4033\n",
      "Epoch: 12/20... Step: 1530... Loss: 1.4527... Val Loss: 1.4061\n",
      "Epoch: 12/20... Step: 1540... Loss: 1.4051... Val Loss: 1.4013\n",
      "Epoch: 12/20... Step: 1550... Loss: 1.4131... Val Loss: 1.3980\n",
      "Epoch: 12/20... Step: 1560... Loss: 1.4229... Val Loss: 1.3969\n",
      "Epoch: 12/20... Step: 1570... Loss: 1.3650... Val Loss: 1.3959\n",
      "Epoch: 12/20... Step: 1580... Loss: 1.3468... Val Loss: 1.3962\n",
      "Epoch: 12/20... Step: 1590... Loss: 1.3498... Val Loss: 1.3944\n",
      "Epoch: 12/20... Step: 1600... Loss: 1.3715... Val Loss: 1.3995\n",
      "Epoch: 12/20... Step: 1610... Loss: 1.3661... Val Loss: 1.3954\n",
      "Epoch: 12/20... Step: 1620... Loss: 1.3590... Val Loss: 1.3878\n",
      "Epoch: 12/20... Step: 1630... Loss: 1.3877... Val Loss: 1.3884\n",
      "Epoch: 12/20... Step: 1640... Loss: 1.3646... Val Loss: 1.3907\n",
      "Epoch: 12/20... Step: 1650... Loss: 1.3410... Val Loss: 1.3869\n",
      "Epoch: 12/20... Step: 1660... Loss: 1.3915... Val Loss: 1.3838\n",
      "Epoch: 13/20... Step: 1670... Loss: 1.3671... Val Loss: 1.3840\n",
      "Epoch: 13/20... Step: 1680... Loss: 1.3748... Val Loss: 1.3816\n",
      "Epoch: 13/20... Step: 1690... Loss: 1.3568... Val Loss: 1.3773\n",
      "Epoch: 13/20... Step: 1700... Loss: 1.3553... Val Loss: 1.3783\n",
      "Epoch: 13/20... Step: 1710... Loss: 1.3283... Val Loss: 1.3751\n",
      "Epoch: 13/20... Step: 1720... Loss: 1.3370... Val Loss: 1.3770\n",
      "Epoch: 13/20... Step: 1730... Loss: 1.3821... Val Loss: 1.3755\n",
      "Epoch: 13/20... Step: 1740... Loss: 1.3418... Val Loss: 1.3719\n",
      "Epoch: 13/20... Step: 1750... Loss: 1.3114... Val Loss: 1.3730\n",
      "Epoch: 13/20... Step: 1760... Loss: 1.3468... Val Loss: 1.3680\n",
      "Epoch: 13/20... Step: 1770... Loss: 1.3607... Val Loss: 1.3672\n",
      "Epoch: 13/20... Step: 1780... Loss: 1.3352... Val Loss: 1.3660\n",
      "Epoch: 13/20... Step: 1790... Loss: 1.3163... Val Loss: 1.3620\n",
      "Epoch: 13/20... Step: 1800... Loss: 1.3442... Val Loss: 1.3616\n",
      "Epoch: 14/20... Step: 1810... Loss: 1.3432... Val Loss: 1.3646\n",
      "Epoch: 14/20... Step: 1820... Loss: 1.3278... Val Loss: 1.3578\n",
      "Epoch: 14/20... Step: 1830... Loss: 1.3599... Val Loss: 1.3609\n",
      "Epoch: 14/20... Step: 1840... Loss: 1.2934... Val Loss: 1.3536\n",
      "Epoch: 14/20... Step: 1850... Loss: 1.2819... Val Loss: 1.3517\n",
      "Epoch: 14/20... Step: 1860... Loss: 1.3336... Val Loss: 1.3501\n",
      "Epoch: 14/20... Step: 1870... Loss: 1.3425... Val Loss: 1.3467\n",
      "Epoch: 14/20... Step: 1880... Loss: 1.3304... Val Loss: 1.3498\n",
      "Epoch: 14/20... Step: 1890... Loss: 1.3474... Val Loss: 1.3486\n",
      "Epoch: 14/20... Step: 1900... Loss: 1.3303... Val Loss: 1.3524\n",
      "Epoch: 14/20... Step: 1910... Loss: 1.3230... Val Loss: 1.3468\n",
      "Epoch: 14/20... Step: 1920... Loss: 1.3260... Val Loss: 1.3444\n",
      "Epoch: 14/20... Step: 1930... Loss: 1.2962... Val Loss: 1.3446\n",
      "Epoch: 14/20... Step: 1940... Loss: 1.3410... Val Loss: 1.3453\n",
      "Epoch: 15/20... Step: 1950... Loss: 1.3122... Val Loss: 1.3428\n",
      "Epoch: 15/20... Step: 1960... Loss: 1.3132... Val Loss: 1.3393\n",
      "Epoch: 15/20... Step: 1970... Loss: 1.2962... Val Loss: 1.3340\n",
      "Epoch: 15/20... Step: 1980... Loss: 1.3004... Val Loss: 1.3404\n",
      "Epoch: 15/20... Step: 1990... Loss: 1.2936... Val Loss: 1.3390\n",
      "Epoch: 15/20... Step: 2000... Loss: 1.2848... Val Loss: 1.3312\n",
      "Epoch: 15/20... Step: 2010... Loss: 1.3031... Val Loss: 1.3291\n",
      "Epoch: 15/20... Step: 2020... Loss: 1.3150... Val Loss: 1.3310\n",
      "Epoch: 15/20... Step: 2030... Loss: 1.2915... Val Loss: 1.3270\n",
      "Epoch: 15/20... Step: 2040... Loss: 1.2950... Val Loss: 1.3287\n",
      "Epoch: 15/20... Step: 2050... Loss: 1.2801... Val Loss: 1.3331\n",
      "Epoch: 15/20... Step: 2060... Loss: 1.2923... Val Loss: 1.3265\n",
      "Epoch: 15/20... Step: 2070... Loss: 1.3071... Val Loss: 1.3287\n",
      "Epoch: 15/20... Step: 2080... Loss: 1.2931... Val Loss: 1.3235\n",
      "Epoch: 16/20... Step: 2090... Loss: 1.2979... Val Loss: 1.3370\n",
      "Epoch: 16/20... Step: 2100... Loss: 1.2829... Val Loss: 1.3236\n",
      "Epoch: 16/20... Step: 2110... Loss: 1.2815... Val Loss: 1.3182\n",
      "Epoch: 16/20... Step: 2120... Loss: 1.2815... Val Loss: 1.3229\n",
      "Epoch: 16/20... Step: 2130... Loss: 1.2686... Val Loss: 1.3240\n",
      "Epoch: 16/20... Step: 2140... Loss: 1.2784... Val Loss: 1.3188\n",
      "Epoch: 16/20... Step: 2150... Loss: 1.2960... Val Loss: 1.3211\n",
      "Epoch: 16/20... Step: 2160... Loss: 1.2756... Val Loss: 1.3173\n",
      "Epoch: 16/20... Step: 2170... Loss: 1.2749... Val Loss: 1.3230\n",
      "Epoch: 16/20... Step: 2180... Loss: 1.2676... Val Loss: 1.3155\n",
      "Epoch: 16/20... Step: 2190... Loss: 1.2913... Val Loss: 1.3163\n",
      "Epoch: 16/20... Step: 2200... Loss: 1.2715... Val Loss: 1.3139\n",
      "Epoch: 16/20... Step: 2210... Loss: 1.2395... Val Loss: 1.3148\n",
      "Epoch: 16/20... Step: 2220... Loss: 1.2820... Val Loss: 1.3128\n",
      "Epoch: 17/20... Step: 2230... Loss: 1.2528... Val Loss: 1.3148\n",
      "Epoch: 17/20... Step: 2240... Loss: 1.2755... Val Loss: 1.3152\n",
      "Epoch: 17/20... Step: 2250... Loss: 1.2557... Val Loss: 1.3143\n",
      "Epoch: 17/20... Step: 2260... Loss: 1.2572... Val Loss: 1.3093\n",
      "Epoch: 17/20... Step: 2270... Loss: 1.2699... Val Loss: 1.3155\n",
      "Epoch: 17/20... Step: 2280... Loss: 1.2726... Val Loss: 1.3078\n",
      "Epoch: 17/20... Step: 2290... Loss: 1.2693... Val Loss: 1.3076\n",
      "Epoch: 17/20... Step: 2300... Loss: 1.2334... Val Loss: 1.3052\n",
      "Epoch: 17/20... Step: 2310... Loss: 1.2588... Val Loss: 1.3056\n",
      "Epoch: 17/20... Step: 2320... Loss: 1.2556... Val Loss: 1.3020\n",
      "Epoch: 17/20... Step: 2330... Loss: 1.2453... Val Loss: 1.3052\n",
      "Epoch: 17/20... Step: 2340... Loss: 1.2600... Val Loss: 1.3058\n",
      "Epoch: 17/20... Step: 2350... Loss: 1.2655... Val Loss: 1.3020\n",
      "Epoch: 17/20... Step: 2360... Loss: 1.2756... Val Loss: 1.3001\n",
      "Epoch: 18/20... Step: 2370... Loss: 1.2440... Val Loss: 1.3015\n",
      "Epoch: 18/20... Step: 2380... Loss: 1.2486... Val Loss: 1.2987\n",
      "Epoch: 18/20... Step: 2390... Loss: 1.2558... Val Loss: 1.3014\n",
      "Epoch: 18/20... Step: 2400... Loss: 1.2733... Val Loss: 1.3006\n",
      "Epoch: 18/20... Step: 2410... Loss: 1.2651... Val Loss: 1.2976\n",
      "Epoch: 18/20... Step: 2420... Loss: 1.2493... Val Loss: 1.2960\n",
      "Epoch: 18/20... Step: 2430... Loss: 1.2573... Val Loss: 1.3013\n",
      "Epoch: 18/20... Step: 2440... Loss: 1.2322... Val Loss: 1.2972\n",
      "Epoch: 18/20... Step: 2450... Loss: 1.2395... Val Loss: 1.2939\n",
      "Epoch: 18/20... Step: 2460... Loss: 1.2495... Val Loss: 1.2972\n",
      "Epoch: 18/20... Step: 2470... Loss: 1.2491... Val Loss: 1.2957\n",
      "Epoch: 18/20... Step: 2480... Loss: 1.2412... Val Loss: 1.2910\n",
      "Epoch: 18/20... Step: 2490... Loss: 1.2382... Val Loss: 1.2922\n",
      "Epoch: 18/20... Step: 2500... Loss: 1.2396... Val Loss: 1.2905\n",
      "Epoch: 19/20... Step: 2510... Loss: 1.2409... Val Loss: 1.2913\n",
      "Epoch: 19/20... Step: 2520... Loss: 1.2560... Val Loss: 1.2900\n",
      "Epoch: 19/20... Step: 2530... Loss: 1.2565... Val Loss: 1.2891\n",
      "Epoch: 19/20... Step: 2540... Loss: 1.2675... Val Loss: 1.2934\n",
      "Epoch: 19/20... Step: 2550... Loss: 1.2238... Val Loss: 1.2959\n",
      "Epoch: 19/20... Step: 2560... Loss: 1.2362... Val Loss: 1.2885\n",
      "Epoch: 19/20... Step: 2570... Loss: 1.2284... Val Loss: 1.2950\n",
      "Epoch: 19/20... Step: 2580... Loss: 1.2577... Val Loss: 1.2933\n",
      "Epoch: 19/20... Step: 2590... Loss: 1.2276... Val Loss: 1.2902\n",
      "Epoch: 19/20... Step: 2600... Loss: 1.2213... Val Loss: 1.2859\n",
      "Epoch: 19/20... Step: 2610... Loss: 1.2348... Val Loss: 1.2915\n",
      "Epoch: 19/20... Step: 2620... Loss: 1.2201... Val Loss: 1.2870\n",
      "Epoch: 19/20... Step: 2630... Loss: 1.2234... Val Loss: 1.2823\n",
      "Epoch: 19/20... Step: 2640... Loss: 1.2338... Val Loss: 1.2882\n",
      "Epoch: 20/20... Step: 2650... Loss: 1.2335... Val Loss: 1.2841\n",
      "Epoch: 20/20... Step: 2660... Loss: 1.2369... Val Loss: 1.2844\n",
      "Epoch: 20/20... Step: 2670... Loss: 1.2484... Val Loss: 1.2805\n",
      "Epoch: 20/20... Step: 2680... Loss: 1.2417... Val Loss: 1.2861\n",
      "Epoch: 20/20... Step: 2690... Loss: 1.2297... Val Loss: 1.2804\n",
      "Epoch: 20/20... Step: 2700... Loss: 1.2364... Val Loss: 1.2778\n",
      "Epoch: 20/20... Step: 2710... Loss: 1.2142... Val Loss: 1.2851\n",
      "Epoch: 20/20... Step: 2720... Loss: 1.2042... Val Loss: 1.2825\n",
      "Epoch: 20/20... Step: 2730... Loss: 1.2051... Val Loss: 1.2820\n",
      "Epoch: 20/20... Step: 2740... Loss: 1.1948... Val Loss: 1.2800\n",
      "Epoch: 20/20... Step: 2750... Loss: 1.2058... Val Loss: 1.2798\n",
      "Epoch: 20/20... Step: 2760... Loss: 1.2081... Val Loss: 1.2850\n",
      "Epoch: 20/20... Step: 2770... Loss: 1.2435... Val Loss: 1.2751\n",
      "Epoch: 20/20... Step: 2780... Loss: 1.2612... Val Loss: 1.2797\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "seq_length = 100\n",
    "n_epochs = 20 # start smaller if you are just testing initial behavior\n",
    "\n",
    "# train the model\n",
    "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_wApq1rceEm"
   },
   "source": [
    "## Getting the best model\n",
    "\n",
    "Чтобы настроить гиперпараметры на максимальную производительность, вам нужно будет посмотреть потери при обучении и проверке. Если ваша потеря на обучении намного ниже, чем потеря на тесте, то модель переобучена. Увеличьте регуляризацию или уменьшите число слоев в сети."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GWojDegbsvWK"
   },
   "source": [
    "В ячейке ниже вы можете увидеть рекомендации, которые были найдены в статьях по оптимизации RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xH_QUEmbceEm"
   },
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Гиперпараметры сети:\n",
    "\n",
    "* `n_hidden` - Количество юнитов в скрытых слоях.\n",
    "* `n_layers` - Количество используемых скрытых слоев LSTM.\n",
    "\n",
    "В нашем примере вероятность отсева и скорость обучения сохраняется.\n",
    "\n",
    "Для обучения:\n",
    "* `batch_size` - количество объектов в батче, проходящих по сети за один проход.\n",
    "* `seq_length` - Количество символов в последовательности, на которой обучается сеть. Обычно чем больше, тем лучше, сеть будет изучать более дальние зависимости.\n",
    "* `lr` - learning rate.\n",
    "\n",
    "\n",
    " ## Советы и хитрости\n",
    "\n",
    "> -  В глубоком обучении очень распространено запускать множество различных моделей с множеством различных настроек гиперпараметров и, в конце концов, использовать любую контрольную точку, дающую наилучшую производительность проверки.\n",
    "\n",
    "> - Кстати, размер ваших тренировочных и проверочных разделов также является параметрами. Убедитесь, что у вас есть приличный объем данных в вашем наборе проверки, иначе производительность проверки будет шумной и не очень информативной."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZfZxvNoDceEm"
   },
   "source": [
    "## Checkpoint\n",
    "\n",
    "После обучения сохраним модель, чтобы можно было загрузить ее позже. Здесь сохраняются параметры, необходимые для создания той же архитектуры, гиперпараметры скрытого слоя и токены."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "q6RXl5VAceEm"
   },
   "outputs": [],
   "source": [
    "# change the name, for saving multiple files\n",
    "model_name = 'rnn_x_epoch.net'\n",
    "\n",
    "checkpoint = {'n_hidden': net.n_hidden,\n",
    "              'n_layers': net.n_layers,\n",
    "              'state_dict': net.state_dict(),\n",
    "              'tokens': net.chars}\n",
    "\n",
    "with open(model_name, 'wb') as f:\n",
    "    torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K2sJhx5iceEm"
   },
   "source": [
    "---\n",
    "## Making Predictions\n",
    "\n",
    "Теперь, когда модель обучена, сделаем предсказание следующих символов! Для предсказания мы передаем последний символ обучения, и сеть предсказывает следующий символ, который мы потом передаем обратно и получаем еще один предсказанный символ и так далее...\n",
    "\n",
    "Наши прогнозы основаны на категориальном распределении вероятностей по всем возможным символам. Мы можем ограничить число символов, чтобы сделать получаемый предсказанный текст более разумным, рассматривая только некоторые наиболее вероятные символы $K$. Это не позволит сети выдавать нам совершенно абсурдные символы, а также позволит внести некоторый шум и случайность в выбранный текст. Узнать больше [можно здесь](https://pytorch.org/docs/stable/torch.html#torch.topk).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "QEIRW_B2ceEm"
   },
   "outputs": [],
   "source": [
    "def predict(net, char, h=None, top_k=None):\n",
    "        ''' Given a character, predict the next character.\n",
    "            Returns the predicted character and the hidden state.\n",
    "        '''\n",
    "        \n",
    "        # tensor inputs\n",
    "        x = np.array([[net.char2int[char]]])\n",
    "        x = one_hot_encode(x, len(net.chars))\n",
    "        inputs = torch.from_numpy(x)\n",
    "        \n",
    "        if(train_on_gpu):\n",
    "            inputs = inputs.cuda()\n",
    "        \n",
    "        # detach hidden state from history\n",
    "        h = tuple([each.data for each in h])\n",
    "        # get the output of the model\n",
    "        out, h = net(inputs, h)\n",
    "\n",
    "        # get the character probabilities\n",
    "        p = F.softmax(out, dim=1).data\n",
    "        if(train_on_gpu):\n",
    "            p = p.cpu() # move to cpu\n",
    "        \n",
    "        # get top characters\n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(net.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "        # select the likely next character with some element of randomness\n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "        \n",
    "        # return the encoded value of the predicted char and the hidden state\n",
    "        return net.int2char[char], h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OG38j3gQceEm"
   },
   "source": [
    "### Priming and generating text \n",
    "\n",
    "Нужно создать скрытое состояние, чтобы сеть не генерировала произвольные символы. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "P9vpB5gRceEm"
   },
   "outputs": [],
   "source": [
    "def sample(net, size, prime='The', top_k=None):\n",
    "        \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "    \n",
    "    net.eval() # eval mode\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = predict(net, ch, h, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "BqmFA9eEceEm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anna had now to say, and he was there it was a past. At that she was thinking of his boy and always back at that sort. There was a cannoticater immediate\n",
      "all to her\n",
      "this stream.\n",
      "\n",
      "Seryozha saw that he had been coughing their conversation and the stepan or anyone\n",
      "with the\n",
      "promited, and\n",
      "she sent him and starting, with his figure at the moment of\n",
      "his face, before.\n",
      "\n",
      "Sergey Ivanovitch with another children with the part were being set all asked, without taking off his stay.\n",
      "\n",
      "Stepan Arkadyevitch's well so talked that he sat down at the\n",
      "most frillly weary, before her excitement when she said, when she went on the same sociecy,\n",
      "was the passions of a last pluch. At the cold matters and head with socioting prince. \"I'll go in and stirll to me...\"\n",
      "\n",
      "\"What don't be the barty of his? And its difeering at the comprehonse of the same man of her house,\" he said at the room,\n",
      "saying the solidiculies to\n",
      "him to stand out in the corried, his hands of his hand, a look, and to think of a performance, which was not \n"
     ]
    }
   ],
   "source": [
    "print(sample(net, 1000, prime='Anna', top_k=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "942mjdQHceEm"
   },
   "source": [
    "## Loading a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xt9ldUuSceEm"
   },
   "outputs": [],
   "source": [
    "# Here we have loaded in a model that trained over 20 epochs `rnn_20_epoch.net`\n",
    "with open('rnn_x_epoch.net', 'rb') as f:\n",
    "    checkpoint = torch.load(f)\n",
    "    \n",
    "loaded = CharRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
    "loaded.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ut6R3zDcceEm"
   },
   "outputs": [],
   "source": [
    "# Sample using a loaded model\n",
    "print(sample(loaded, 2000, top_k=5, prime=\"And Levin said\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TH1ag4h1ceEm"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Verteeva_RNN_Exercise_3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
